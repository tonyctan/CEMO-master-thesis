\chapter{Review of Matrix Calculus}

\setlength{\parindent}{0in}

\section{Notations}

Let us first establish the notation. This is important because bad notation is a serious obstacle to elegant mathematics and coherent exposition and it can be misleading.

Unless specified otherwise, $\phi$ denotes a scalar function; $\m{f}$ a vector function and $\m{F}$ a matrix function. Also, $x$ denotes a scalar argument, $\m{x}$ a vector argument and $\m{X}$ a matrix argument. For example, we write

\begin{table*}[h]
  \begin{center}
  \begin{tabular}{lll}
    $\phi(x)=x^2$     &$\phi(\m{x})=\T{a}\m{x}$       &$\phi(\m{X})=\tr{\T{X}\m{X}}$\\
    $\m{f}(x)=
      \begin{pmatrix}
        x\\
        x^2
      \end{pmatrix}$    &$\m{f}(\m{x})=\m{Ax}$    &$\m{f}(\m{X})=\m{Xa}$\\
      $\m{F}(x)=x^2\Id{m}$  &$\m{F}(\m{x})=\m{x}\T{x}$  &$\m{F}(\m{X})=\T{X}$
  \end{tabular}
\end{center}
\end{table*}

Since the prime notation $'$ may easily cause confusion between derivatives and transposes, preference is given to the Leibniz notation $\frac{\dd}{\dd x}$ for derivatives and $\T{}$ for transposes---unless this system becomes too cumbersome, in which case $\m{f}'(\m{x})$ will denote derivatives and $\m{f}(\m{x})'$ for transposes.

\section{Derivatives and differentials}

\subsection{Derivative}

\theoremstyle{definition}
\begin{definition}[Derivatives]\label{Def.D}
  If $\m{f}$ is an $m\times 1$ vector function of an $n\times 1$ vector $\m{x}$, then the \emph{derivative} (or \emph{Jacobian matrix}) of $\m{f}$ is the $m\times n$ matrix
  \begin{equation}\label{Eq.Def.D}
    \D\m{f}(\m{x}):=\frac{\partial\m{f}(\m{x})}{\partial\T{x}},
  \end{equation}
  whose elements are the partial derivatives
  \begin{equation*}
    \frac{\partial f_i(\m{x})}{\partial x_j},\ \text{for}\ %
    \begin{aligned}
      i&=1,\cdots,m,\\
      j&=1,\cdots,n.
    \end{aligned}
  \end{equation*}
\end{definition}

\subsection{Differential}

In the one dimensional case, the equation
\begin{equation}\label{Eq.D}
  \lim_{u\to 0}\frac{\phi(x+u)-\phi(x)}{u}=\phi'(x)
\end{equation}
defines the derivative of $\phi$ at $x$. Rewriting \cref{Eq.D} gives
\begin{equation}\label{Eq.d}
  \phi(x+u)=\phi(x)+\phi'(x)u+O(u),
\end{equation}
where the remainder term $O(u)$ quickly vanishes as $u$ approaches $0$.

\theoremstyle{definition}
\begin{definition}[Differential]
  We define the (first) \emph{differential} of $\phi$ at $x$ (with increment $u$) as
  \begin{equation}
    \dd\phi(x;u)=\phi'(x)u.
  \end{equation}
\end{definition}

For example, for $\phi(x)=x^2$, we have $\dd\phi(x;u)=2xu$. In practice, we write $\dd x$ instead of $u$, so that $\dd\phi(x)=\phi'(x)\dd x=2x\dd x.$

In the vector case, similar to \cref{Eq.d}, we have
\begin{equation}
  \m{f}(\m{x}+\m{u})=\m{f}(\m{x})+[\D\m{f}(x)]\m{u}+O(\m{u}),
\end{equation}
and the (first) differential is defined as
\begin{equation}
  \dd\m{f}(\m{x};\m{u})=[\D\m{f}(x)]\m{u}.
\end{equation}

Although rarely used in econometrics, for completeness, the matrix case can be obtained from the vector case by writing $\m{f}:=\vec{F}$ and $\m{x}:=\vec{X}$.

\subsection{Which to use?}

For practical rather than theoretical reasons, the treatment of matrix calculus is based on differentials ($\dd\m{f}$) rather than derivatives ($\D\m{f}$) because the former yields a result with the same dimension as $\m{f}$. For example, consider $\md{f}{m}{1}(\md{x}{n}{1})$ (reading ``$\m{f}$ being an $m\times 1$ vector function of an $n\times 1$ vector $\m{x}$''), $\D\m{f}(\m{x})$ is an $m\times n$ matrix (due to \cref{Def.D}) whereas $\dd\m{f}(\m{x})$ remains an $m\times 1$ vector (same as $\m{f}$). The advantage is even larger for matrices: for $\md{F}{m}{p}(\md{X}{n}{q})$, $\dd\m{F}(\m{X})$ has the same dimension as $\m{F}$ irrespective of the dimension of $\m{X}$, but $\D\m{F}(\m{X})$ is going to be a horrendous $mp\times nq$ matrix.

\section{Layout convention}\label{S.layout}

Under the \emph{numerator layout}, when we differentiate a scalar function $\phi$ \wrt a column vector $\md{x}{n}{1}$, we get a \emph{row} vector of dimension $1\times n$. If we want our result to be in the column form, we must differentiate $\phi$ \wrt a row vector to start with. This is why the denominator in \cref{Eq.Def.D} contains a transpose.

\section{Application in OLS}

\subsection{Background}

Imagine we are interested in learning the return on education. We might propose a rather simple model
\begin{equation}
  \texttt{inc}=\beta_0+\beta_1\texttt{edu}+\beta_2\texttt{exp}+\epsilon
\end{equation}
where \texttt{inc} is one's income, \texttt{edu} and \texttt{exp} denote years of formal education and years spent in the labour market, respectively.

We managed to collect survey data from $n$ respondents and organised this information in the following system of equations:
\begin{equation}
  \left\{
    \begin{aligned}
      \texttt{inc}_1 &= \beta_0+\beta_1\texttt{edu}_1+\beta_2\texttt{exp}_1+\epsilon_1\\
      \texttt{inc}_2 &= \beta_0+\beta_1\texttt{edu}_2+\beta_2\texttt{exp}_2+\epsilon_2\\
      \cdots\\
      \texttt{inc}_n &= \beta_0+\beta_1\texttt{edu}_n+\beta_2\texttt{exp}_n+\epsilon_n\\
    \end{aligned}
  \right.
\end{equation}

This system of linear equations can be represented in the matrix notation using
\begin{equation}
  \md{y}{n}{1}=
    \begin{pmatrix}
      \texttt{inc}_1\\
      \texttt{inc}_2\\
      \cdots\\
      \texttt{inc}_n\\
    \end{pmatrix},\ %
  \md{X}{n}{3}=
    \begin{pmatrix}
      1     &\texttt{edu}_1     &\texttt{exp}_1\\
      1     &\texttt{edu}_2     &\texttt{exp}_2\\
      \cdots\\
      1     &\texttt{edu}_n     &\texttt{exp}_n\\
    \end{pmatrix},\ %
  \md{\beta}{3}{1}=
    \begin{pmatrix}
      \beta_0\\
      \beta_1\\
      \beta_2\\
    \end{pmatrix},\ \text{and}\ %
  \md{\epsilon}{n}{1}=
    \begin{pmatrix}
      \epsilon_1\\
      \epsilon_2\\
      \cdots\\
      \epsilon_n\\
    \end{pmatrix}
  \end{equation}
as
\begin{equation}\label{Eq.setup}
  \m{y}=\m{X\beta}+\m{\epsilon}.
\end{equation}

\subsection{Ordinary least squares}

The objective of OLS is to minimise the \emph{sum of squared} error terms. A handy way of representing sum of squared $\epsilon$ is
\begin{equation}
  \text{SSE}=\sum_{i=1}^n\epsilon_i^2=\epsilon_1^2+\epsilon_2^2+\cdots+\epsilon_n^2=
  \begin{pmatrix}
    \epsilon_1      &\epsilon_2     &\cdots      &\epsilon_n
  \end{pmatrix}
  \begin{pmatrix}
    \epsilon_1\\
    \epsilon_2\\
    \cdots\\
    \epsilon_n
  \end{pmatrix}
  =\T{\epsilon}\m{\epsilon}.
\end{equation}
In fact, $\T{x}\m{x}$ is the mathematical translation of ``sum of squared'' of $\m{x}$.

Now we are ready to continue. We want to carefully choose a combination of $\beta_0$, $\beta_1$ and $\beta_2$ in order to make SSE as small as possible, ie
\begin{equation}\label{Eq.min}
  \min{\T{\epsilon}\m{\epsilon}}{\m{\beta}}=\min{\left(\m{y}-\m{X\beta}\right)\Ts\left(\m{y}-\m{X\beta}\right)}{\m{\beta}}
\end{equation}
(the equal sign is due to \cref{Eq.setup}).

Two observations can be made from the minimisation problem in \cref{Eq.min}:
\begin{enumerate}
  \item both $\m{y}$ and $\m{X}$ are collected data therefore can no longer be changed by the researcher; but we are free to adjust $\m{\beta}$ in whatever way we want, meaning $\m{\beta}$ is the ``independent variable'' and SSE is a function of $\m{\beta}$, and
  \item $\T{\epsilon}\m{\epsilon}$ is a scalar function (please verify).
\end{enumerate}
Then,
\begin{equation}
  \begin{aligned}
    \phi(\m{\beta})=\T{\epsilon}\m{\epsilon}&=\left(\m{y}-\m{X\beta}\right)\Ts\left(\m{y}-\m{X\beta}\right)\\
  &=\left(\T{y}-\T{\beta}\T{X}\right)\left(\m{y}-\m{X\beta}\right)\\
  &=\T{y}\m{y}-\T{y}\m{X\beta}-\T{\beta}\T{X}\m{y}+\T{\beta}\T{X}\m{X\beta}
  \end{aligned}
\end{equation}

We now differentiate $\phi(\m{\beta})$ \wrt $\m{\beta}$:
\begin{equation}\label{Eq.normal}
  \begin{aligned}
    \frac{\dd \phi(\m{\beta})}{\dd\m{\beta}}&=-\T{y}\m{X}-\frac{\dd}{\dd\m{\beta}}\left[\left(\T{\beta}\T{X}\m{y}\right)\Ts\right]+\T{\beta}\T{X}\m{X}+\frac{\dd}{\dd\m{\beta}}\left[\left(\T{\beta}\T{X}\m{X\beta}\right)\Ts\right]\\
    &=-\T{y}\m{X}-\frac{\dd}{\dd\m{\beta}}\left[\T{y}\m{X}\m{\beta}\right]+\T{\beta}\T{X}\m{X}+\frac{\dd}{\dd\m{\beta}}\left[\T{\beta}\T{X}\m{X}\m{\beta}\right]\\
    &=-\T{y}\m{X}-\T{y}\m{X}+\T{\beta}\T{X}\m{X}+\T{\beta}\T{X}\m{X}\\
    &=-2\T{y}\m{X}+2\T{\beta}\T{X}\m{X}
  \end{aligned}
\end{equation}
(We were able to liberally apply transpose to terms containing $\T{\beta}$ and not to others because $\phi$ is a scalar function and each term in it must also be $1\times 1$ in dimension, whose transpose must be equal to itself.)

Apply first order condition to \cref{Eq.normal}. An optimal $\mhat{\beta}$ must satisfy
\begin{equation}\label{Eq.FOC}
  \begin{aligned}
    -2\T{y}\m{X}+2\mhat{\beta}\Ts\T{X}\m{X}&=\Z\\
    2\mhat{\beta}\Ts\T{X}\m{X}&=2\T{y}\m{X}\\
    \mhat{\beta}\Ts\T{X}\m{X}&=\T{y}\m{X}\\
    \left(\mhat{\beta}\Ts\T{X}\m{X}\right)\Ts&=\left(\T{y}\m{X}\right)\Ts\\
    \T{X}\m{X}\mhat{\beta}&=\T{X}\m{y}\\
    \mhat{\beta}&=\left( \T{X}\m{X} \right)^{-1} \T{X}\m{y}
  \end{aligned}
\end{equation}

Notice that another transpose was applied to Line 4 of \cref{Eq.FOC} in order to correct $\mhat{\beta}\Ts$ (due to \cref{S.layout}) back to its column form $\mhat{\beta}$. In fact, it would be better to do $\frac{\dd\phi(\m{\beta})}{\dd\T{\beta}}$ in \cref{Eq.normal} to avoid this later flipping. But the downside of this approach is a pedagogical one: most students would find differentiating \wrt $\T{\beta}$ out of blue while \wrt $\m{\beta}$ is much more natural. In further derivations, $\frac{\dd\phi(\m{\beta})}{\dd\T{\beta}}$ will be used.

\newpage

\begin{know}{Derivative of quadratic forms}
  \abovedisplayshortskip=0pt
  \belowdisplayshortskip=0pt
  \abovedisplayskip=0pt
  \belowdisplayskip=0pt
The derivative of a quadratic form $q(\m{x}) = \T{x}\m{A}\m{x}$ is
\begin{equation*}
  \frac{\dd q}{\dd \m{x}} = \T{x} \left( \m{A} + \T{A} \right),
\end{equation*}
which can be further simplified to $ \dd q / \dd \m{x} = 2\T{x}\m{A}$, if $\m{A}$ is symmetric.
\end{know}

Name the expression in the facebook post $\phi$, which is a function of $\m{\beta}$:
\begin{equation*}
  \phi(\m{\beta}) = \frac{1}{\sigma^2}\left( \m{y} - \m{X}\m{\beta} \right)\Ts \inv{\Omega} \m{Z} \left( \T{Z} \inv{\Omega} \m{Z} \right)^{-1} \T{Z} \inv{\Omega} \left( \m{y} - \m{X}\m{\beta} \right).
\end{equation*}

\begin{caution}{Typo}
There is a typo in the original post: all $\m{\Omega}$ should be in the inverse form $\inv{\Omega}$, including the one sandwiched between $\T{Z}$ and $\m{Z}$.
\end{caution}

\begin{note}{Scalar function}
  \abovedisplayshortskip=0pt
  \belowdisplayshortskip=0pt
  \abovedisplayskip=0pt
  \belowdisplayskip=0pt
Note that $\phi$ is a scalar function:
\begin{equation*}
  \underset{1 \times 1}{\phi} \left( \md{\beta}{k}{1} \right) = \frac{1}{\sigma^2} \left( \md{y}{n}{1} - \md{X}{n}{k} \md{\beta}{k}{1} \right)\Ts \md{\Omega}{n}{n}^{-1} \md{Z}{n}{k} \left( \md{Z}{k}{n}\Ts \md{\Omega}{n}{n}^{-1} \md{Z}{n}{k} \right)^{-1} \md{Z}{k}{n}\Ts \md{\Omega}{n}{n}^{-1} \left( \md{y}{n}{1} - \md{X}{n}{k} \md{\beta}{k}{1} \right).
\end{equation*}

\bigskip

When differentiating a scalar function $\phi$ \wrt a column vector $\m{\beta}$, the result is a \emph{row} vector. If this is undesirable, differentiate the scalar function $\phi$ \wrt the \emph{transpose} of that vector $\T{\beta}$.
\end{note}

I want to know
\begin{equation*}
  \frac{\dd \phi}{\dd \m{\beta}} = \frac{\dd \phi}{\dd \left( \m{y} - \m{X}\m{\beta} \right)} \frac{\dd \left( \m{y} - \m{X}\m{\beta} \right)}{\dd \m{\beta}} = \frac{\dd \phi}{\dd \left( \m{y} - \m{X}\m{\beta} \right)} \left( -\m{X} \right),
\end{equation*}
so I first calculate (using the result from quadratic form derivatives stated at the beginning)
\begin{equation*}
    \frac{\dd \phi}{\dd \left( \m{y} - \m{X}\m{\beta} \right)} =
      \frac{2}{\sigma^2} \left( \m{y} - \m{X}\m{\beta} \right)\Ts \left[ \inv{\Omega} \m{Z} \left( \T{Z} \inv{\Omega} \m{Z} \right)^{-1} \T{Z} \inv{\Omega} \right].
\end{equation*}

Therefore,
\begin{equation*}
  \begin{aligned}
    \frac{\dd \phi}{\dd \m{\beta}} &= -\frac{2}{\sigma^2}\left( \m{y} - \m{X}\m{\beta} \right)\Ts \left[ \inv{\Omega} \m{Z} \left( \T{Z} \inv{\Omega} \m{Z} \right)^{-1} \T{Z} \inv{\Omega} \right] \m{X}\\
    \frac{\dd \phi}{\dd \T{\beta}} &= -\frac{2}{\sigma^2} \T{X} \left[ \inv{\Omega} \m{Z} \left( \T{Z} \inv{\Omega} \m{Z} \right)^{-1} \T{Z} \inv{\Omega} \right] \left( \m{y} - \m{X}\m{\beta} \right)
  \end{aligned}
\end{equation*}
(The second line is to avoid working with row vectors.)

\begin{note}{Second derivative}
  \abovedisplayshortskip=0pt
  \belowdisplayshortskip=0pt
  \abovedisplayskip=0pt
  \belowdisplayskip=0pt
The second derivative of $\phi$ is
\begin{equation*}
  \frac{\dd^2 \phi}{\dd \T{\beta} \dd \m{\beta}} = \frac{2}{\sigma^2} \T{X} \inv{\Omega} \m{Z} \left( \T{Z} \inv{\Omega} \m{Z} \right)^{-1} \T{Z} \inv{\Omega} \m{X},
\end{equation*}
which is a positive definite $k \times k$ matrix (another quadratic form). This implies that the result from the first order condition below is a minimum.
\end{note}

Impose the first order condition:
\begin{equation*}
  \begin{aligned}
    \T{X} \inv{\Omega} \m{Z} \left( \T{Z} \inv{\Omega} \m{Z} \right)^{-1} \T{Z} \inv{\Omega} \left( \m{y} - \m{X}\mhat{\beta}_{\text{GLS-IV}} \right) &= \m{0}\\
    \T{X} \inv{\Omega} \m{Z} \left( \T{Z} \inv{\Omega} \m{Z} \right)^{-1} \T{Z} \inv{\Omega} \m{y} &= \T{X} \inv{\Omega} \m{Z} \left( \T{Z} \inv{\Omega} \m{Z} \right)^{-1} \T{Z} \inv{\Omega} \m{X} \mhat{\beta}_{\text{GLS-IV}}
  \end{aligned}
\end{equation*}

Therefore:
\begin{equation*}
  \begin{aligned}
  \mhat{\beta}_{\text{GLS-IV}}
  &= \left( \T{X} \inv{\Omega} \m{Z} \left( \T{Z} \inv{\Omega} \m{Z} \right)^{-1} \T{Z} \inv{\Omega} \m{X} \right)^{-1} \T{X} \inv{\Omega} \m{Z} \left( \T{Z} \inv{\Omega} \m{Z} \right)^{-1} \T{Z} \inv{\Omega} \m{y}\\
  &= \left( \T{X} \inv{\Omega} \mhat{X} \right)^{-1} \T{X} \inv{\Omega} \mhat{y}\\
  &\neq \left( \mhat{X}\Ts \inv{\Omega} \mhat{X} \right)^{-1} \mhat{X}\Ts \inv{\Omega} \m{y},\ \text{satisfying the claim in the facebook post.}
  \end{aligned}
\end{equation*}

\bigskip

However, since $\mhat{X}$ is the GLS-IV-estimator of $\m{X}$ onto the $\m{Z}$-space:
\begin{equation*}
  \begin{aligned}
    \mhat{X} &= \m{Z} \left( \T{Z} \inv{\Omega} \m{Z} \right)^{-1} \T{Z} \inv{\Omega} \m{X},\ \text{and}\\
    \mhat{X}\Ts &= \T{X} \inv{\Omega} \m{Z} \left( \T{Z} \inv{\Omega} \m{Z} \right)^{-1} \T{Z}.
  \end{aligned}
\end{equation*}

The last expression in the facebook post then becomes:
\begin{equation*}
  \begin{aligned}
      &\left( \mhat{X}\Ts \inv{\Omega} \mhat{X} \right)^{-1} \mhat{X}\Ts \inv{\Omega} \m{y}\\
      = &\left( \T{X} \inv{\Omega} \m{Z} \left( \T{Z} \inv{\Omega} \m{Z} \right)^{-1} \T{Z} \inv{\Omega} \m{Z} \left( \T{Z} \inv{\Omega} \m{Z} \right)^{-1} \T{Z} \inv{\Omega} \m{X} \right)^{-1} \T{X} \inv{\Omega} \m{Z} \left( \T{Z} \inv{\Omega} \m{Z} \right)^{-1} \T{Z} \inv{\Omega} \m{y}\\
      = & \left( \T{X} \inv{\Omega} \m{Z} \left( \T{Z} \inv{\Omega} \m{Z} \right)^{-1} \T{Z} \inv{\Omega} \m{X} \right)^{-1} \T{X} \inv{\Omega} \m{Z} \left( \T{Z} \inv{\Omega} \m{Z} \right)^{-1} \T{Z} \inv{\Omega} \m{y}\\
      = &\mhat{\beta}_{\text{GLS-IV}}
  \end{aligned}
\end{equation*}

After all, ``one might have guessed'' correctly!
